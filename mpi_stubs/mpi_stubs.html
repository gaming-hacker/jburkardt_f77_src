<html>

  <head>
    <title>
      MPI_STUBS - Dummy MPI Library
    </title>
  </head>

  <body bgcolor="#EEEEEE" link="#CC0000" alink="#FF3300" vlink="#000055">

    <h1 align = "center">
      MPI_STUBS <br> Dummy MPI Library
    </h1>

    <hr>

    <p>
      <b>MPI_STUBS</b>
      is a FORTRAN77 library which
      implements "stub" versions of MPI routines.
    </p>

    <p>
      <b>MPI_STUBS</b> is intended to include stubs for the most commonly
      called MPI routines.  Most of the stub routines don't do anything.
      In a few cases, where it makes sense, they do some simple action
      or return a value that is appropriate for the serial processing
      case.
    </p>

    <p>
      <b>MPI_STUBS</b> can be used as a convenience, when a real MPI
      implementation is not available, and the user simply wants to
      test-compile a code.  It may also be useful in those occasions
      when a code has been so carefully written that it will still
      execute correctly on a single processor.
    </p>

    <p>
      <b>MPI_STUBS</b> is based on a similar package supplied as
      part of the <b>LAMMPS</b> program, which allow that program to
      be compiled, linked and run on a single processor machine,
      although it is normally intended for parallel execution.
    </p>

    <h3 align = "center">
      Licensing:
    </h3>

    <p>
      The computer code and data files described and made available on this web page
      are distributed under
      <a href = "../../txt/gnu_lgpl.txt">the GNU LGPL license.</a>
    </p>

    <h3 align = "center">
      Languages:
    </h3>

    <p>
      <b>MPI_STUBS</b> is available in
      <a href = "../../c_src/mpi_stubs/mpi_stubs.html">a C version</a> and
      <a href = "../../cpp_src/mpi_stubs/mpi_stubs.html">a C++ version</a> and
      <a href = "../../f77_src/mpi_stubs/mpi_stubs.html">a FORTRAN77 version</a> and
      <a href = "../../f_src/mpi_stubs/mpi_stubs.html">a FORTRAN90 version</a>.
    </p>

    <h3 align = "center">
      Related Data and Programs:
    </h3>

    <p>
      <a href = "../../f77_src/heat_mpi/heat_mpi.html">
      HEAT_MPI</a>,
      a FORTRAN77 program which
      solves the 1D time dependent heat equation using the finite difference
      method, with parallelization from MPI.
    </p>

    <p>
      <a href = "../../f77_src/hello_mpi/hello_mpi.html">
      HELLO_MPI</a>,
      a FORTRAN77 program which
      prints out "Hello, world!" using the MPI parallel programming environment.
    </p>

    <p>
      <a href = "../../examples/moab/moab.html">
      MOAB</a>,
      examples which
      illustrate the use of the MOAB job scheduler for a computer cluster.
    </p>

    <p>
      <a href = "../../f77_src/mpi/mpi.html">
      MPI</a>,
      FORTRAN77 programs which
      illustrate the use of the MPI library of message passing routines which
      enables parallel processing on a variety of machine architectures, and with
      a varying number of processors.
    </p>

    <p>
      <a href = "../../f77_src/multitask_mpi/multitask_mpi.html">
      MULTITASK_MPI</a>,
      a FORTRAN77 program which
      demonstrates how to "multitask", that is, to execute several unrelated
      and distinct tasks simultaneously, using MPI for parallel execution.
    </p>

    <p>
      <a href = "../../f77_src/quad_mpi/quad_mpi.html">
      QUAD_MPI</a>,
      a FORTRAN77 program which
      approximates an integral using a quadrature rule, and carries out the
      computation in parallel using MPI.
    </p>

    <p>
      <a href = "../../f77_src/satisfy_mpi/satisfy_mpi.html">
      SATISFY_MPI</a>,
      a FORTRAN77 program which
      demonstrates, for a particular circuit, an exhaustive search
      for solutions of the circuit satisfiability problem, using MPI to
      carry out the calculation in parallel.
    </p>

    <h3 align = "center">
      Reference:
    </h3>

    <p>
      <ol>
        <li>
          William Gropp, Steven Huss-Lederman, Andrew Lumsdaine, Ewing Lusk,
          Bill Nitzberg, William Saphir, Marc Snir,<br>
          MPI: The Complete Reference,<br>
          Volume II: The MPI-2 Extensions,<br>
          Second Edition,<br>
          MIT Press, 1998,<br>
          ISBN13: 978-0-262-57123-4,<br>
          LC: QA76.642.M65.
        </li>
      </ol>
    </p>

    <h3 align = "center">
      Source Code:
    </h3>

    <p>
      <ul>
        <li>
          <a href = "mpi_stubs.f">mpi_stubs.f</a>, the source code.
        </li>
        <li>
          <a href = "mpi_stubs_f77.h">mpi_stubs_f77.h</a>, the "include" file.
        </li>
        <li>
          <a href = "mpi_stubs.sh">mpi_stubs.sh</a>,
          commands to compile the source code.
        </li>
      </ul>
    </p>

    <h3 align = "center">
      Examples and Tests:
    </h3>

    <p>
      <b>BUFFON_LAPLACE</b> is an "embarassingly parallel" program
      which carries out a randomized series of Buffon-Laplace trials.
      <ul>
        <li>
          <a href = "buffon_laplace.f">buffon_laplace.f</a>,
          a sample calling program.
        </li>
        <li>
          <a href = "buffon_laplace.sh">buffon_laplace.sh</a>,
          commands to compile and run the program.
        </li>
        <li>
          <a href = "buffon_laplace_output.txt">buffon_laplace_output.txt</a>,
          the output file.
        </li>
      </ul>
    </p>

    <p>
      <b>QUADRATURE</b> is a program that estimates an integral
      using the random sampling.
      <ul>
        <li>
          <a href = "quadrature.f">quadrature.f</a>,
          a sample calling program.
        </li>
        <li>
          <a href = "quadrature.sh">quadrature.sh</a>,
          commands to compile and run the program.
        </li>
        <li>
          <a href = "quadrature_output.txt">quadrature_output.txt</a>,
          the output file.
        </li>
      </ul>
    </p>

    <h3 align = "center">
      List of Routines:
    </h3>

    <p>
      <ul>
        <li>
          <b>MPI_ALLGATHER</b> gathers data from all the processes in a communicator.
        </li>
        <li>
          <b>MPI_ALLGATHERV</b> gathers data from all the processes in a communicator.
        </li>
        <li>
          <b>MPI_ALLREDUCE</b> carries out a reduction operation.
        </li>
        <li>
          <b>MPI_BARRIER</b> forces processes within a communicator to wait together.
        </li>
        <li>
          <b>MPI_BCAST</b> broadcasts data from one process to all others.
        </li>
        <li>
          <b>MPI_CART_CREATE</b> creates a communicator for a Cartesian topology.
        </li>
        <li>
          <b>MPI_CART_GET</b> returns the "Cartesian coordinates" of the calling process.
        </li>
        <li>
          <b>MPI_CART_SHIFT</b> finds the destination and source for Cartesian shifts.
        </li>
        <li>
          <b>MPI_COMM_FREE</b> "frees" a communicator.
        </li>
        <li>
          <b>MPI_COMM_RANK</b> reports the rank of the calling process.
        </li>
        <li>
          <b>MPI_COMM_SIZE</b> reports the number of processes in a communicator.
        </li>
        <li>
          <b>MPI_COMM_SPLIT</b> splits up a communicator based on a key.
        </li>
        <li>
          <b>MPI_COPY_DOUBLE</b> copies a double precision vector.
        </li>
        <li>
          <b>MPI_COPY_INTEGER</b> copies an integer vector.
        </li>
        <li>
          <b>MPI_COPY_REAL</b> copies a real vector.
        </li>
        <li>
          <b>MPI_FINALIZE</b> shuts down the MPI library.
        </li>
        <li>
          <b>MPI_GET_COUNT</b> reports the actual number of items transmitted.
        </li>
        <li>
          <b>MPI_INIT</b> initializes the MPI library.
        </li>
        <li>
          <b>MPI_IRECV</b> receives data from another process.
        </li>
        <li>
          <b>MPI_ISEND</b> sends data from one process to another using nonblocking transmission.
        </li>
        <li>
          <b>MPI_RECV</b> receives data from another process within a communicator.
        </li>
        <li>
          <b>MPI_REDUCE_SCATTER</b> collects a message of the same length from each process.
        </li>
        <li>
          <b>MPI_RSEND</b> "ready sends" data from one process to another.
        </li>
        <li>
          <b>MPI_SEND</b> sends data from one process to another.
        </li>
        <li>
          <b>MPI_WAIT</b> waits for an I/O request to complete.
        </li>
        <li>
          <b>MPI_WAITALL</b> waits until all I/O requests have completed.
        </li>
        <li>
          <b>MPI_WAITANY</b> waits until one I/O requests has completed.
        </li>
        <li>
          <b>MPI_WTIME</b> returns the elapsed wall clock time.
        </li>
      </ul>
    </p>

    <p>
      You can go up one level to <a href = "../f77_src.html">
      the FORTRAN77 source codes</a>.
    </p>

    <hr>

    <i>
      Last revised on 26 October 2007.
    </i>

    <!-- John Burkardt -->

  </body>

</html>
